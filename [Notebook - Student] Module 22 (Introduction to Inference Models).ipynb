{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Inference Models\n",
    "\n",
    "\n",
    "To navigate up and down, you can use the up and down arrow keys on your keyboard<br />\n",
    "To execute code in this workbook, select the code block and press **Shift+Enter** <br />\n",
    "To edit the code block, press enter. \n",
    "\n",
    "#### The codes in this workbook are cumulative. (Variables defined continue to be available until the notebook is closed) <br />\n",
    "So do start from the top and work your way down to avoid unexpected results!\n",
    "\n",
    "\n",
    "For more help on using Jupyter Notebook, you can click on Help > User Interface Tour in the menu above, <br />\n",
    "or visit https://jupyter-notebook.readthedocs.io/en/stable/ui_components.html\n",
    "\n",
    "Experiment and test out your ideas, for that is one of the fastest ways to learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What can machine learning models predict?\n",
    "\n",
    "In the last session, you trained a model that took in an image (as input), and returned a prediction of which class it belonged to (as output). That was a form of **classification**. Some of you went further to determine where the cards were within the image. That was a form of **localization**. And some of you went on to classify which class each individual pixel in the image belonged to. That was a form of **image segmentation**.\n",
    "\n",
    "In general, machine learning models take feature vectors as inputs. For computer vision, these feature vectors are calculated from the images. They are then fed into the model which consequently predicts a set of output. These outputs may include:\n",
    "\n",
    "- For **image classification**, a class label for the image\n",
    "- For **localization**, a class label for the image, and coordinates for the bounding box.\n",
    "- For **object detection**, a list of bounding boxes (and class labels for object recognition) for objects detected.\n",
    "- For **image segmentation**, a class label for each pixel in the image\n",
    "- The output may also include the probability scores for the different class labels.\n",
    "\n",
    "Today's session will introduce Convolutional Neural Networks (CNN), and then go on to explore how to make use of pre-trained models for classification and object detection in the real world.\n",
    "1. What are Convolutional Neural Networks\n",
    "1. How to use the Intel® OpenVINO™ Inference Engine with OpenCV\n",
    "1. How to make inferences on the Intel Neural Compute Stick 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last session, we made use of basic machine learning algorithms such as kNN and SVM. In recent years, Convolutional Neural Networks (CNN) have gained great popularity due to its effectiveness. CNNs are a form of Neural Networks that are made up of a series of convolution filters. And the network learns the optimal filters (also known as weights) through the process of training. \n",
    "\n",
    "**What are convolutions?**\n",
    "\n",
    "In simple terms, convolutions are a mathematical operations performed on the input to produce an output. It can also be used as a form of feature extraction. \n",
    "\n",
    "Convolutions can also be used for basic image processing. For example, blurring, sharpening, averaging, and bilateral filtering.\n",
    "\n",
    "For CNNs, the model learns the optimal weights each of the convolution operations during training (instead of having them manually defined). Optimization techniques such as gradient descent are used (together with back propagation of errors) in order to calculate the optimal \"weights\" for each of the convolutions. We will not be going into the detailed implementation or math in this session, but you are always encouraged to [learn more](http://deeplearning.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Convolutions\n",
    "\n",
    "Let us just apply a few convolution filters on a photograph of a printed circuit board (PCB) to somewhat visualize the concept of convolutions. **cv2.filter2D** is an image processing function in OpenCV that can be used to perform convolutions https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/filter_2d/filter_2d.html\n",
    "\n",
    "<img src=\"[Dataset]_Module22_images/convolutions.png\" />\n",
    "\n",
    "Notice how the words and circuit board patterns are clearer and sharper after some convolution filters are applied, compared to others. Does it remind you of thresholding in the earlier session? Convolutions can be used to extract meaningful features from input images. And in the context of CNNs, a series of convolutions are typically used (and not just 1).\n",
    "\n",
    "Let us work through some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Let's quickly recap the code to read in an image\n",
    "image = cv2.imread(\"[Dataset]_Module22_images/pcb.png\")\n",
    "\n",
    "# We convert it to greyscale to illustrate 2D convolutions.\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display the greyscale image\n",
    "cv2.imshow(\"Original Greyscale Image\",gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to blur this image using a convolution filter as follows. How does the result compare to the original?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel1 = np.ones((19,19),np.float32)/361      # a 19x19 convolution filter\n",
    "filter1 = cv2.filter2D(gray,-1,kernel1)\n",
    "\n",
    "cv2.imshow(\"Original Greyscale Image\",gray)\n",
    "cv2.imshow(\"Filter1\",filter1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel2 = np.array((                        # a 3x3 convolution filter\n",
    "    [0, -1, 0],\n",
    "    [-1, 4, -1],\n",
    "    [0, -1, 0]), np.float32)*2\n",
    "filter2 = cv2.filter2D(gray,-1,kernel2)\n",
    "\n",
    "cv2.imshow(\"Original Greyscale Image\",gray)\n",
    "cv2.imshow(\"Filter2\",filter2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the 3x3 array of numbers in the code above? That is the convolution filter that is used to perform the mathematical operations on the input image. [Read more](http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/#convolutions) to see how the image can be transformed to a convolved feature using a filter like this. The example in the link illustrates the resulting feature vector output as smaller than the input. However, in our examples, the output maintains the same size as the input because cv2.filter2D adds a padding automatically.\n",
    "\n",
    "Let's try 1 more filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel3 = np.array((\n",
    "    [-2, -2, 0],\n",
    "    [-2, 1, 2],\n",
    "    [0, 2, 2]), dtype=\"int\")           \n",
    "filter3 = cv2.filter2D(gray,-1,kernel3)  \n",
    "\n",
    "cv2.imshow(\"Original\",gray)\n",
    "cv2.imshow(\"Filter3\",filter3)      #Does the output image remind you of filters in Photoshop?\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the 3x3 array of numbers again? In the context of CNNs, those numbers are known as weights of the respective convolutional (CONV) layers which the CNN will learn as part of the model training process.\n",
    "\n",
    "Convolutional Neural Networks typically contain a series of convolutional layers (CONV), and are used together with Activation and Pooling Layers. Activation layers are usually applied after every CONV layer and Pooling layers are used to reduce the spatial dimensions. Dropout layers may also be added to prevent overfitting. The detailed implementation is outside the scope of this session, but you are encouraged to [read more](http://cs231n.github.io/convolutional-networks/#layers)\n",
    "\n",
    "When training is complete, the CNN can be used to make predictions just like other machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Intel&reg; OpenVINO&trade; Inference Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now switch gears to use some pre-trained CNNs. We heard about CNNs, let us now see them in action. the Intel&reg; OpenVINO&trade; Toolkit ships together with some pre-trained models that you can use. You can also use the Model Optimizer to convert models trained on other frameworks such as Caffe, MxNet, and Tensorflow. The toolkit currently supports running the optimized models on either CPU, GPU, FPGA or MYRIAD (the NCS2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model Optimizer is available at:\n",
    "%OPENVINO_INSTALL_DIR%/Intel/computer_vision_sdk/deployment_tools/model_optimizer \n",
    "\n",
    "You can run the model optimizer by activating the cv environment (where you installed Python 3.6) <br />\n",
    "and run **python mo.py** with the required arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the FP32 data type, more commonly used for CPUs:\n",
    "(replace %PATH_TO_MODEL% and %PATH_TO_OUTPUT% accordingly)\n",
    "\n",
    "python mo.py --input_model \"%PATH_TO_MODEL%\\squeezenet1.1.caffemodel\" --output_dir \"%PATH_TO_OUTPUT%/FP32\" --data_type FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the FP16 data type, more commonly used for NCS2:\n",
    "\n",
    "(replace %PATH_TO_MODEL% and %PATH_TO_OUTPUT% accordingly)\n",
    "\n",
    "python mo.py --input_model \"%PATH_TO_MODEL%\\squeezenet1.1.caffemodel\" --output_dir \"%PATH_TO_OUTPUT%/FP16\" --data_type FP16\n",
    "\n",
    "\n",
    "To give you a smoother start, some models have already been converted for you in the \"models\" subdirectory\n",
    "\n",
    "Bonus: You can learn more about FP16 and FP32 [here](https://www.quora.com/What-is-the-difference-between-FP16-and-FP32-when-doing-deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Sample Model for Classification (running on CPU)\n",
    "\n",
    "Documentation for the Intel® OpenVINO™ Python API is available [online](https://software.intel.com/en-us/articles/OpenVINO-InferEngine#overview-of-inference-engine-python-api)\n",
    "\n",
    "As of 18 January 2019, the documentation says \"**NOTE: This is a preview version of the Inference Engine Python* API for evaluation purpose only. Module structure and API itself will be changed in future releases.**\"\n",
    "\n",
    "Hence, it is likely that the API will be updated eventually. For now, let us explore how to use it.\n",
    "\n",
    "The Intel® OpenVINO™ installation also comes with a sample python file that performs classification. You can find the file at %OPENVINO_INSTALL_DIR%/Intel/computer_vision_sdk/inference_engine/python_samples/classification_sample.py \n",
    "\n",
    "Alternatively, you can execute the code below, and the results will be displayed. It will first show the neural network being loaded, and then the classification results (beginning with the class labels that have the highest probability).\n",
    "\n",
    "### Classifying between 1000 different image classes with SqueezeNet\n",
    "\n",
    "This model has been trained to recognize images from 1000 classes. You can read more about the project  [here](https://arxiv.org/abs/1602.07360).\n",
    "\n",
    "Next, let us try to make use of this model to make some classifications.\n",
    "\n",
    "Apart from the example below, you can also try a few other images such as images/dog.jpeg, images/hamster.jpeg and images/bluecar.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Creating Inference Engine\n",
      "[ INFO ] Loading network:\n",
      "\tC:\\Users\\HEFRY ANESTI\\Desktop\\training\\squeezenet1.1\\FP32\\squeezenet1.1.xml\n",
      "[ INFO ] Preparing input blobs\n",
      "[ WARNING ] Image [Dataset]_Module22_images/cat.jpeg is resized from (1500, 2034) to (227, 227)\n",
      "[ INFO ] Batch size is 1\n",
      "[ INFO ] Loading model to the plugin\n",
      "[ INFO ] Start inference (10 Asynchronous executions)\n",
      "[ INFO ] Completed 1 Async request execution\n",
      "[ INFO ] Completed 2 Async request execution\n",
      "[ INFO ] Completed 3 Async request execution\n",
      "[ INFO ] Completed 4 Async request execution\n",
      "[ INFO ] Completed 5 Async request execution\n",
      "[ INFO ] Completed 6 Async request execution\n",
      "[ INFO ] Completed 7 Async request execution\n",
      "[ INFO ] Completed 8 Async request execution\n",
      "[ INFO ] Completed 9 Async request execution\n",
      "[ INFO ] Completed 10 Async request execution\n",
      "[ INFO ] Processing output blob\n",
      "[ INFO ] Top 10 results: \n",
      "Image [Dataset]_Module22_images/cat.jpeg\n",
      "\n",
      "classid probability\n",
      "------- -----------\n",
      "  cat     0.7695209\n",
      "tabby cat 0.1498846\n",
      "  cat    0.0361377\n",
      "catamount 0.0187889\n",
      "  cat    0.0087181\n",
      "terrier  0.0051658\n",
      "fox, Vulpes vulpes0.0016991\n",
      "  vat    0.0013994\n",
      "terrier  0.0010574\n",
      "cat, Siamese0.0008215\n",
      "\n",
      "\n",
      "[ INFO ] This sample is an API example, for any performance measurements please use the dedicated benchmark_app tool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python classification_sample.py -i [Dataset]_Module22_images/cat.jpeg -m \"C:\\Users\\HEFRY ANESTI\\Desktop\\training\\squeezenet1.1\\FP32\\squeezenet1.1.xml\" --labels \"C:\\Users\\HEFRY ANESTI\\Desktop\\training\\squeezenet1.1\\FP32\\squeezenet1.1.labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Classify at least 1 more photo from the folder ./images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do try out more images and explore why some images might be misclassified, especially those with more objects in the background. \n",
    "\n",
    "Next, we will be exploring how to run the same model on the NCS2, a portable USB device that is designed to speed up inference tasks on edge devices. It can also be used on your laptop.\n",
    "\n",
    "\n",
    "### 2.2 Sample model for Classification (running on NCS2)\n",
    "\n",
    "The Neural Compute Stick is intended to make inferences a breeze, especially for larger networks. Let us see how we can run our inference on the NCS2 instead of on our CPU.\n",
    "\n",
    "First, you need to plugin in the NCS2 into a USB port on your computer. If you have a USB3.0 port, use it for faster data transfer speeds. Then you just need to execute the code below.\n",
    "\n",
    "How different is it from the code above?\n",
    "\n",
    "We have added **-d MYRIAD** to specify to use the NCS2 instead of the CPU. (Myriad is actually the name of the powerful chip that is inside the NCS2) And we have also updated the model path to the FP16 directory instead of the FP32 directory. This is because the NCS2 only supports **FP16**, while CPUs tend to use FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Creating Inference Engine\n",
      "[ INFO ] Loading network:\n",
      "\tsqueezenet1.1/FP16/squeezenet1.1.xml\n",
      "[ INFO ] Preparing input blobs\n",
      "[ WARNING ] Image [Dataset]_Module22_images/cat.jpeg is resized from (1500, 2034) to (227, 227)\n",
      "[ INFO ] Batch size is 1\n",
      "[ INFO ] Loading model to the plugin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HEFRY ANESTI\\Desktop\\training\\classification_sample.py\", line 175, in <module>\n",
      "    sys.exit(main() or 0)\n",
      "  File \"c:\\Users\\HEFRY ANESTI\\Desktop\\training\\classification_sample.py\", line 133, in main\n",
      "    exec_net = ie.load_network(network=net, device_name=args.device)\n",
      "  File \"ie_api.pyx\", line 403, in openvino.inference_engine.ie_api.IECore.load_network\n",
      "  File \"ie_api.pyx\", line 442, in openvino.inference_engine.ie_api.IECore.load_network\n",
      "RuntimeError: Can not init Myriad device: NC_ERROR\n"
     ]
    }
   ],
   "source": [
    "!python classification_sample.py -d MYRIAD -i \"[Dataset]_Module22_images/cat.jpeg\" -m \"squeezenet1.1/FP16/squeezenet1.1.xml\" --labels \"squeezenet1.1/FP16/squeezenet1.1.labels\"\n",
    "#In case not using NCS2 please replace \"MYRIAD\" with \"CPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Classify at least 1 more photo from the folder ./images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities generated on the CPU vs. NCS2 might have some differences due to the way that floating points are supported/calculated. Remeber that we used the FP16 model for the NCS2 and the FP32 model for the CPU. These 2 models are not exactly the same. If you look at the model file sizes in the models subdirectory, you would notice that the .bin file for the FP16 model tends to be much smaller than the .bin file for the FP32 model.\n",
    "\n",
    "Comparing the results on your CPU and on the NCS2, you might only find minor improvements in running time for this Network as it is a small network that runs quickly within 10-20ms. However, as we will see later on, there are some of the Intel Pre-Trained models that can run on the NCS2 that are currently not supported via CPU. But before we go down the rabbit hole, let us pause here and take a look at the code inside classification_sample.py to understand the logic.\n",
    "\n",
    "\n",
    "**After using the model, let us dig deeper into the code**\n",
    "\n",
    "As the code was originally written in C++ for greater efficiency, python wrappers have been created and these are the IEPlugin and IENetwork modules from the openvino.inference_engine package\n",
    "\n",
    "As a start, you might want to open up classification_sample.py to understand the code. The general idea is to:\n",
    "1. Initialize the hardware via IEPlugin (CPU / Myriad / GPU / FPGA)\n",
    "2. Load the network via IENetwork\n",
    "3. Preprocess the input image into the required format\n",
    "4. Make the prediction via exec_net.infer\n",
    "5. Processes/Display the output\n",
    "6. Release Memory. i.e. del plugin, del net and del exec_net. While the del keyword is less commonly seen in python applications, it seems to be required in order to release the load on the NCS2 (when run from the Jupyter Notebook). When it is run directly via command line, there might be automatic release once the script finishes execution.   \n",
    "\n",
    "Before proceeding, do open \"classification_sample.py\" to briefly read through the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Using a Python Class to make things easier\n",
    "\n",
    "Often, we do not really want to implement the lower level codes over and over again. If you recall the previous session, once the model was loaded from the sklearn package, there was an awesome method called **model.predict()**. \n",
    "\n",
    "Similarly, can we can just load the CNN model, and then use something similar to model.predict to generate the predictions?\n",
    "\n",
    "Yes, we can.\n",
    "\n",
    "Let us use a helper class from https://github.com/simpledevelopments/OpenVINO-Python-Utils \n",
    "The package has been tested to work on the 2018R5 release of the Intel&reg; OpenVINO&trade; Toolkit. As the Inference Engine Python API is subject to change, our code might need to be updated eventually when the time comes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from utils.opv import OpvModel\n",
    "      # Helper class for OpenVINO 2021.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Machine Released\n",
      "[INFO] Model squeezenet1.1 Loaded and Ready on NCS device 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HEFRY ANESTI\\Desktop\\training\\utils\\opv.py:86: DeprecationWarning: 'inputs' property of IENetwork class is deprecated. To access DataPtrs user need to use 'input_data' property of InputInfoPtr objects which can be accessed by 'input_info' property.\n",
      "  self.input_layer = next(iter(net.inputs))\n"
     ]
    }
   ],
   "source": [
    "# If you encounter the error \"Can not init USB device: NC_DEVICE_NOT_FOUND\", \n",
    "#   check that the NCS2 is plugged into your USB port. \n",
    "#\n",
    "# After you no longer need mymodel to make any more inferences, \n",
    "#   run mymodel.ClearMachine() to release the application from the device.\n",
    "\n",
    "mymodel = OpvModel(\"squeezenet1.1\", device=\"CPU\", fp=\"FP16\", ncs=1, debug=True)\n",
    "#In case not using NCS2 please replace \"MYRIAD\" with \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[INFO] Image resized from (1500, 2034) to (227, 227)\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"[Dataset]_Module22_images/cat.jpeg\")\n",
    "predictions = mymodel.Predict(image)\n",
    "reimage = cv2.resize(image, (620, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After you no longer need mymodel to make any more inferences, \n",
    "#   you can run mymodel.ClearMachine() to release the application from the device.\n",
    "#   or wait for automatic cleanup when the python application closes. \n",
    "#   (for Jupyter Notebook, that happens when you use File > Close and Halt)\n",
    "\n",
    "#mymodel.ClearMachine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.0% chance: Persian cat\n",
      "15.0% chance: tabby, tabby cat\n",
      "3.6% chance: tiger cat\n",
      "1.9% chance: lynx, catamount\n",
      "0.9% chance: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "def displayresults(predictions, labels=None):\n",
    "    if (labels is None):                         # If no labels provided, use numerical labels\n",
    "        labels = np.arange(0,1000).astype(\"str\")\n",
    "    predictions = predictions.reshape(1000)\n",
    "    top5 = predictions.argsort()[-5:][::-1]\n",
    "    top5results = np.column_stack([predictions[top5], labels[top5]])\n",
    "    for (conf, label) in top5results:\n",
    "        calcu = round(float(conf)*100, 1)\n",
    "        print(str(calcu)+\"% chance: \"+label)\n",
    "        \n",
    "        if calcu >= 70.0:\n",
    "            cv2.putText(reimage, f\"{label}\", (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 1)\n",
    "            cv2.imshow(\"prediction\", reimage)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "displayresults(predictions ,mymodel.labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results with section 2.2 that was also run on the NCS2. The results should match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note: Revision on NumPy array.\n",
    "See below to find out how to sort and subset NumPy arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 2 1]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4, 5, 2, 1])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4 3 2 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "np.flip(np.sort(a))\n",
    "print (np.flip(np.sort(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4]\n"
     ]
    }
   ],
   "source": [
    "np.flip(np.sort(a))[:2]\n",
    "print (np.flip(np.sort(a))[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4]\n"
     ]
    }
   ],
   "source": [
    "a[a.argsort()[-2:][::-1]]\n",
    "print(a[a.argsort()[-2:][::-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.1 How were those results neatly formatted?**\n",
    "\n",
    "Are you wondering what that code did? Let us go step by step,starting from the predictions\n",
    "\n",
    "We are expecting 1000 records, each displaying the probability of the image belonging to that particular class. But predictions.shape shows our array in the form (1,1000,1,1). Hence the first step to reshape it to a 1D array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.reshape(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.2 How do we get the array indices for the top 5 results?**\n",
    "\n",
    "To learn more about numpy, visit https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.argsort.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([283, 281, 282, 287, 285], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argsort()[-5:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.3 What are the probabilities for each of these \"top\" predictions?**\n",
    "\n",
    "They are represented in decimal format (1 means 100% confidence. 0.1 means 10% confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7695209 , 0.14988461, 0.0361377 , 0.01878894, 0.00871808],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[predictions.argsort()[-5:][::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.4 And what are the class labels for these predictions?**\n",
    "\n",
    "For this particular example, when we loaded the model, the labels for the different classes were also loaded from the squeezenet1.1.labels (inside the models/squeezenet1.1/FP16 subdirectory) Hence, we can access it using mymodel.labels. Let's just print the first 5 records. You can cross check against the squeezenet1.1.labels file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tench, Tinca tinca' 'goldfish, Carassius auratus'\n",
      " 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias'\n",
      " 'tiger shark, Galeocerdo cuvieri' 'hammerhead, hammerhead shark']\n"
     ]
    }
   ],
   "source": [
    "print(mymodel.labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.5 Great, but how do we apply the labels to the predictions?**\n",
    "\n",
    "Here are the top 5 results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Persian cat', 'tabby, tabby cat', 'tiger cat', 'lynx, catamount',\n",
       "       'Egyptian cat'], dtype='<U121')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.labels[predictions.argsort()[-5:][::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.6 Now how might we combine both the probabilities and the results into an array?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0.7695209', 'Persian cat'],\n",
       "       ['0.14988461', 'tabby, tabby cat'],\n",
       "       ['0.036137696', 'tiger cat'],\n",
       "       ['0.018788941', 'lynx, catamount'],\n",
       "       ['0.008718076', 'Egyptian cat']], dtype='<U121')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5 = predictions.argsort()[-5:][::-1]\n",
    "np.column_stack([    predictions[top5],    mymodel.labels[top5]     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2.3.7 Here's everything put together again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1400 into shape (1000,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HEFRYA~1\\AppData\\Local\\Temp/ipykernel_1112/3259280227.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop5results\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"% chance: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdisplayresults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmymodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\HEFRYA~1\\AppData\\Local\\Temp/ipykernel_1112/3259280227.py\u001b[0m in \u001b[0;36mdisplayresults\u001b[1;34m(predictions, labels)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m                         \u001b[1;31m# If no labels provided, use numerical labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"str\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtop5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtop5results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtop5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtop5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1400 into shape (1000,)"
     ]
    }
   ],
   "source": [
    "def displayresults(predictions, labels=None):\n",
    "    if (labels is None):                         # If no labels provided, use numerical labels\n",
    "        labels = np.arange(0,1000).astype(\"str\")\n",
    "    predictions = predictions.reshape(1000)\n",
    "    top5 = predictions.argsort()[-5:][::-1]\n",
    "    top5results = np.column_stack([predictions[top5], labels[top5]])\n",
    "    for (conf, label) in top5results:\n",
    "        print(str(round(float(conf)*100,1))+\"% chance: \"+label)\n",
    "displayresults(predictions,mymodel.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Putting it all together, make predictions about another image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! You have successfully performed classification, making use of the pre-trained SqueezeNet1.1 and OpenVINO. Take some time to think about the process of loading the models, performing the classification and displaying the results. You can write your notes in your student activity guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Library of Inference Models\n",
    "\n",
    "Remember that there are different types of inference models. Some do image classification, some do object detection or even image segmentation. Intel also provides a set of pre-trained inference models together with the Intel® OpenVINO™ installation. You can find out more about the pre-trained models at\n",
    "https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models\n",
    "\n",
    "**Optimizing models into the IR format**<br />\n",
    "At the beginning of section 2, we briefly talked about the Model Optimizer. Before Trained Models are run on the Intel® OpenVINO™ Inference Engine, they need to be optimized into an Intermediate Representation (IR) format. In this way, models trained on frameworks such as Caffe, MxNet, and Tensorflow can be converted into the IR format that can be efficiently run using the toolkit. For more details, refer to https://software.intel.com/en-us/articles/OpenVINO-ModelOptimizer\n",
    "\n",
    "**Pre-Trained models in IR format**<br />\n",
    "The good news is, for the pre-trained Intel models, they have already been converted into the IR format which means they are ready to be used straight out of the box. These can be found at %OPENVINO_INSTALL_DIR%/Intel/computer_vision_sdk/deployment_tools/intel_models\n",
    "\n",
    "\n",
    "To use any of those models, you can just copy the respective subfolders into the models subdirectory where this notebook is located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Face Detection using a model from the Intel Models \"Library\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do digital cameras detect peoples' faces? Can we also do something like that?\n",
    "\n",
    "<img src=\"[Dataset]_Module22_images/friendsfaces.png\" style=\"width:400px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make use of the **face-detection-adas-0001** model to help us do just that!\n",
    "\n",
    "### Task: Refer to the model documentation, note down the input and output of the model. \n",
    "\n",
    "You can refer to the documentation for this model at %OPENVINO_INSTALL_DIR%/Intel/computer_vision_sdk/deployment_tools/intel_models/face-detection-adas-0001/description/face-detection-adas-0001.html\n",
    "\n",
    "The network (model) outputs a blob with the shape: [1, 1, N, 7], where N is the number of detected bounding boxes. For each detection, the description has the format: [image_id, label, conf, x_min, y_min, x_max, y_max] (See the 7 variables? That accounts for the 7 in [1,1,N,7])\n",
    "\n",
    "**Copy the folder \"face-detection-adas-0001\" from intel_models into your models folder, then let's get started!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HEFRY ANESTI\\Desktop\\training\\utils\\opv.py:86: DeprecationWarning: 'inputs' property of IENetwork class is deprecated. To access DataPtrs user need to use 'input_data' property of InputInfoPtr objects which can be accessed by 'input_info' property.\n",
      "  self.input_layer = next(iter(net.inputs))\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from utils.opv import OpvModel\n",
    "\n",
    "mymodel2 = OpvModel(\"face-detection-adas-0001\", device=\"CPU\", fp=\"FP16\", ncs=1)\n",
    "#In case not using NCS2 please replace \"MYRIAD\" with \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = cv2.imread(\"[Dataset]_Module22_images/friends.jpeg\")\n",
    "friends = cv2.resize(friends,(1200,710))     #Downsize the image since the original is quite large\n",
    "predictions = mymodel2.Predict(friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 200, 7)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the shape you've obtained with the output shape presented in the documentation document. You should get the same shape!\n",
    "\n",
    "How many predictions are there, can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.        , 1.        , 1.        , ..., 0.39807257,\n",
       "          0.84158474, 0.5066318 ],\n",
       "         [0.        , 1.        , 0.99999964, ..., 0.4601197 ,\n",
       "          0.39078203, 0.5425858 ],\n",
       "         [0.        , 1.        , 0.995613  , ..., 0.4436372 ,\n",
       "          0.5696492 , 0.5426169 ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are 121 entries\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do you wonder what is included in the predictions variable? \n",
    "\n",
    "### Task: List down at least 3 result of the prediction. Notice how they are labelled [image_id, label, conf, x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, it is now time to draw our bounding box! Try the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawBoundingBoxes(predictions, image, conf=0):\n",
    "    canvas = image.copy()                             # copy instead of modifying the original image\n",
    "    predictions_1 = predictions[0][0]                 # subset dataframe\n",
    "    confidence = predictions_1[:,2]                   # getting conf value [image_id, label, conf, x_min, y_min, x_max, y_max]\n",
    "    topresults = predictions_1[(confidence>conf)]     # choosing only predictions with conf value bigger than treshold\n",
    "    (h,w) = canvas.shape[:2]                        # \n",
    "    for detection in topresults:\n",
    "        box = detection[3:7] * np.array([w, h, w, h]) # determine box location\n",
    "        (xmin, ymin, xmax, ymax) = box.astype(\"int\") # assign box location value to xmin, ymin, xmax, ymax\n",
    "\n",
    "        cv2.rectangle(canvas, (xmin, ymin), (xmax, ymax), (0, 0, 255), 4)  # make a rectangle\n",
    "        cv2.putText(canvas, str(round(detection[2]*100,1))+\"%\", (xmin, ymin), # include text\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0,0), 2)\n",
    "    cv2.putText(canvas, str(len(topresults))+\" face(s) detected\", (50,50), # include text\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0,0), 2)\n",
    "    return canvas\n",
    "\n",
    "cv2.imshow(\"Friends2\",DrawBoundingBoxes(predictions,friends))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh, seems that we are drawing far too many boxes. It appears that we have detected 200 bounding boxes. However, not every bounding box may be a face. To see which are valid predictions, we will need to compare the probability scores against a threshold that we set. \n",
    "\n",
    "### Task: Use 50% confidence as our threshold. \n",
    "\n",
    "Recall how we displayed the results for the classification example, and compare how the code below works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawBoundingBoxes(predictions, image, conf=0.5):\n",
    "    canvas = image.copy()                             # copy instead of modifying the original image\n",
    "    predictions_1 = predictions[0][0]                 # subset dataframe\n",
    "    confidence = predictions_1[:,2]                   # getting conf value [image_id, label, conf, x_min, y_min, x_max, y_max]\n",
    "    topresults = predictions_1[(confidence>conf)]     # choosing only predictions with conf value bigger than treshold\n",
    "    (h,w) = canvas.shape[:2]                        # setting the variable h and w according to image height\n",
    "    \n",
    "    #\n",
    "    for detection in topresults:\n",
    "        box = detection[3:7] * np.array([w, h, w, h]) # determine box location\n",
    "        (xmin, ymin, xmax, ymax) = box.astype(\"int\") # assign box location value to xmin, ymin, xmax, ymax\n",
    "\n",
    "        cv2.rectangle(canvas, (xmin, ymin), (xmax, ymax), (0, 0, 255), 4)  # make a rectangle\n",
    "        cv2.putText(canvas, str(round(detection[2]*100,1))+\"%\", (xmin, ymin), # include text\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0,0), 2)\n",
    "    cv2.putText(canvas, str(len(topresults))+\" face(s) detected\", (50,50), # include text\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0,0), 2)\n",
    "    return predictions_1, canvas\n",
    "\n",
    "predictions_1,x = DrawBoundingBoxes(predictions,friends)\n",
    "cv2.imshow(\"Friends2\",x)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what do you see? You see that only our friends' faces are put in the box!\n",
    "\n",
    "Now, time to test your understanding of the code. \n",
    "\n",
    "### Task: Print our confidence variable. What does this variable contain? Can you notice the confidence level referring to the four friends on the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Print topresults variable. What does it contain? \n",
    "Identify again [image_id, label, conf, x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Print out detection [3:7]\n",
    "What does it refer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: print the variable box. \n",
    "What does it refer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, you will proceed with drawing the bounding box and the text label. \n",
    "\n",
    "### Task: Draw the bounding box again with your own settings. \n",
    "This time, change the color and thickness of the box, and the size and type of the font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawBoundingBoxes(predictions, image, conf=0.5):\n",
    "    canvas = image.copy()                             # copy instead of modifying the original image\n",
    "    predictions_1 = predictions[0][0]                 # subset dataframe\n",
    "    confidence = predictions_1[:,2]                   # getting conf value [image_id, label, conf, x_min, y_min, x_max, y_max]\n",
    "    topresults = predictions_1[(confidence>conf)]     # choosing only predictions with conf value bigger than treshold\n",
    "    (h,w) = canvas.shape[:2]                        # setting the variable h and w according to image height\n",
    "    \n",
    "    #\n",
    "    for detection in topresults:\n",
    "        box = detection[3:7] * np.array([w, h, w, h]) # determine box location\n",
    "        (xmin, ymin, xmax, ymax) = box.astype(\"int\") # assign box location value to xmin, ymin, xmax, ymax\n",
    "\n",
    "        cv2.rectangle(canvas, (xmin, ymin), (xmax, ymax), (100, 150, 255), 2)  # make a rectangle\n",
    "        cv2.putText(canvas, str(round(detection[2]*100,1))+\"%\", (xmin, ymin), # include text\n",
    "            cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 230,40), 2)\n",
    "    cv2.putText(canvas, str(len(topresults))+\" friend(s) detected. It's time to party!\", (50,50), # include text\n",
    "            cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 10,110), 2)\n",
    "    return predictions_1, canvas\n",
    "\n",
    "predictions_1,x = DrawBoundingBoxes(predictions,friends)\n",
    "cv2.imshow(\"Friends2\",x)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Use the model to classify and draw a bounding box on another image!\n",
    "Try images with people facing sideways, making funny faces, etc. What do you notice about the ability of the pre-trained model to look for faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have just built your first face detector!\n",
    "\n",
    "## It's now time for you to do some hands on! \n",
    "\n",
    "You have just used 2 very powerful pre-trained models. One does image classification between 1000 classes of objects, and one does face detection.\n",
    "\n",
    "As you explore different pre-trained models provided in the Intel® Distribution of OpenVINO™ Toolkit, look into the intel_models subdirectory and into the description subfolder of the respective models. The documentation there will give you a better idea of what the respective models are capable of predicting. Once you have done the predictions, it is back to making use of your basic image processing skills to display the results.\n",
    "\n",
    "Whatever you build, keep in mind your purpose and objective and think of different possible approaches. As always, also keep in mind the possible impacts when a machine learning algorithm makes a misclassification and plan for ways to mitigate the risks. For example, what is the worst possible consequence of detecting a face when there is actually no face? Or not detecting a face when it is actually there? As with other scenarios where there may be a probability of error, explore complementing the design of your real-world solutions with other techniques or perhaps even hardware sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Blur images/dog.jpeg using what you learnt in section 1\n",
    "Do you also know how to make the blur effect more or less blur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Modify the code in section 2.3 to give the top 3 results instead of top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = OpvModel(\"face-detection-adas-0001\", device=\"MYRIAD\", fp=\"FP16\", ncs=1, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Load the labels from models/squeezenet1.1/FP16/squeezenet1.1.labels into a numpy array. What is the first label inside the file?\n",
    "\n",
    "How to you read the labels from file and load it into a numpy array? <br />\n",
    "Hint: If you get stuck, you might want to check how it is done inside utils/opv.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Use the classification model to classify images/pcb.png. Does it get classified as a printed circuit board (PCB)? Why do you think so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5: Calculate how long it takes to make 1 prediction for the model in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def wastetime():\n",
    "    for i in range(0,10000000):\n",
    "        testing = 1\n",
    "        \n",
    "# Sample code for calculating time difference         \n",
    "a = time.time()\n",
    "wastetime()\n",
    "b = time.time()\n",
    "print(str(b-a) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: Build an application that does real-time face detection via your webcam.\n",
    "\n",
    "Hint: Make use of what you learned in section 4, and what you already know about video capture via webcam from the earlier session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 7: Do the real-time face detections look a little laggy on your screen? Can you think of 1 or more ways to make the detections faster?\n",
    "\n",
    "Hint: Remember that the bigger the image, the more the processing required. Also, not all models are made alike. Think of different possibilities and try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 8: Can I use 2 NCS devices at the same time?\n",
    "\n",
    "If you borrow one more NCS device from your friend, you can try running both the face detection and the classification model at the same time. Set the ncs flag to 1 for the first, and set the ncs flag to 2 for the second model. You can also set the debug flag to True to see the output.\n",
    "\n",
    "For example, <br />\n",
    "mymodel = OpvModel(\"face-detection-retail-0004\", device=\"MYRIAD\", fp=\"FP16\", ncs=1, debug = True) <br />\n",
    "mymodel2 = OpvModel(\"squeezenet1.1\", device=\"MYRIAD\", fp=\"FP16\", ncs=2, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 9: Touch your NCS2. Does it feel hot?\n",
    "\n",
    "To release the NCS2, you will need to release that memory. Automatic [garbage collection](https://docs.python.org/3/library/gc.html) does not happen for the OpvModel yet because mymodel is still defined and the Jupyter Notebook session has not ended. And OpvModel holds a reference to the NCS2 which holds the model that you loaded earlier.\n",
    "\n",
    "To release that reference, you can do:\n",
    "1. mymodel.ClearMachine(), or\n",
    "2. halt and close this notebook, or \n",
    "3. plug out the NCS2 when no longer in use.\n",
    "\n",
    "For safety, it might be a good pratice to plug out the NCS2 from the USB port when not in use to prevent overheating.\n",
    "\n",
    "Also keep a lookout for when the new Python API for the NCS2 might be released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.ClearMachine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 10: Take a look at the Intel Documentation of Pre-Trained Models, how many types of models can you find?\n",
    "You can find the documentation at %OPENVINO_INSTALL_DIR%/Intel/computer_vision_sdk/deployment_tools/\n",
    "intel_models/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look out for the documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are you having fun?\n",
    "\n",
    "We sure hope you are! Be sure to remember to think of ways that you can put what you learn to good use. Remember, with great power comes great responsibility! Use your knowledge to design and develop solutions to help people and society around you! \n",
    "\n",
    "Up to this point, we have only tested out 3 pre-trained models. Be sure to explore the many other pre-trained models that Intel has provided together with the OpenVINO installation!\n",
    "\n",
    "Welcome to the beginning of your exciting journey of learning ahead!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
